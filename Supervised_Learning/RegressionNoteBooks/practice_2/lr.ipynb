{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8376faa",
   "metadata": {},
   "source": [
    "1. minimal and interview-focused implementation of **Linear Regression** in Python using scikit-learn, following the steps you mentioned. It uses the Boston Housing dataset (or California Housing, since Boston is deprecated) and includes all critical steps—cleaning, standardization, splitting, training, evaluation, and optional cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d79b41db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.555891598695244\n",
      "R2 score:  0.5757877060324511\n",
      "Cross Validation R2: 0.5530311140279559\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 1. load dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# 2. Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Split the data into training and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"MSE: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"R2 score: \", r2_score(y_test, y_pred))\n",
    "\n",
    "# 6. Cross-validation\n",
    "scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')\n",
    "print(\"Cross Validation R2:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1d01f",
   "metadata": {},
   "source": [
    "2. minimal yet complete implementation of **Polynomial Regression** using scikit-learn. This script satisfies all the specified steps, with concise code suited for quick interview practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b26e5f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -46.8821475   187.25521723 -117.34997502 -130.6925585   -95.69336681\n",
      "   45.13835534  -50.59336942 -119.73414324   44.89223635   31.01953078\n",
      "  -47.27024402   38.99148066   23.77657436  193.02312659  -89.60039863\n",
      "  -21.14807728   40.00715257 -103.41488773  -11.46905675  175.09628473\n",
      "  131.57855496   98.9430734    33.51626485   46.9638043   -18.1589644\n",
      "    6.58672766    6.45533012   29.61894615  167.05396447   21.18815608\n",
      "   33.55545614   13.8662212    84.78274559 -227.21628727   61.07188727\n",
      "  -22.73593005  -14.6837161  -122.87894644 -139.91520111  -52.1240007 ]\n",
      "MSE:  384.7886908140149\n",
      "R2 score:  0.9565477683395806\n",
      "Cross-validated R2:  0.9279007481463681\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 1. Generate synthetic dataset suitable for polynomial regression\n",
    "X, y = make_regression(n_samples=200, n_features=1, noise=20, random_state=1)\n",
    "y= y + 0.5 * (X[:, 0] ** 2)\n",
    "\n",
    "# 2. Standardize Feature\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Transform to polynomial features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "# 4. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(y_test)\n",
    "\n",
    "# 5. Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"MSE: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"R2 score: \", r2_score(y_test, y_pred))\n",
    "\n",
    "# 7. Cross-validation\n",
    "cv_score = cross_val_score(model, X_poly, y, cv=5, scoring='r2')\n",
    "print(\"Cross-validated R2: \", cv_score.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05187f3c",
   "metadata": {},
   "source": [
    "3. Here’s a minimal yet complete implementation of **Ridge Regression**, designed to meet your requirements for quick interview practice. It uses the California Housing dataset and includes model training, evaluation, cross-validation, and optional hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb56689f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.5558034669932211\n",
      "R2 score:  0.5758549611440126\n",
      "Cross-validated R2:  0.5530382161908947\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1. Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# 2. Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Train model\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"MSE: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"R2 score: \", r2_score(y_test, y_pred))\n",
    "\n",
    "# 6. Cross Validaition\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')\n",
    "print('Cross-validated R2: ', cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac1a2d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha:  10\n",
      "Best Cross-Validated R2:  0.5530925208131402\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(Ridge(), params, cv=5, scoring='r2')\n",
    "grid.fit(X_scaled, y)\n",
    "\n",
    "print(\"Best alpha: \", grid.best_params_['alpha'])\n",
    "print(\"Best Cross-Validated R2: \", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79ae03d",
   "metadata": {},
   "source": [
    "4. minimal and interview-ready implementation of **Lasso Regression** that follows the same structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f6f5277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  1.3106960720039365\n",
      "R2 SCORE:  -0.00021908714592466794\n",
      "Cross_validated R2:  -0.0891728361025553\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 1. Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns= data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# 2. Scale the data \n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# 3. split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. train the model\n",
    "model = Lasso(alpha=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluate model \n",
    "y_pred = model.predict(X_test)\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred))\n",
    "print('R2 SCORE: ', r2_score(y_test, y_pred))\n",
    "\n",
    "# 6. Cross Validation\n",
    "cv_score = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')\n",
    "print(\"Cross_validated R2: \", cv_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6340f492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha:  0.001\n",
      "Best cross-validated r2:  0.553257237003969\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'alpha': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(Lasso(max_iter=1000), params, cv=5, scoring='r2')\n",
    "grid.fit(X_scaled, y)\n",
    "\n",
    "print(\"Best alpha: \", grid.best_params_['alpha'])\n",
    "print(\"Best cross-validated r2: \", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062b387",
   "metadata": {},
   "source": [
    "5. minimal code example that shows feature selection using Lasso Regression, followed by a step-by-step explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8275232d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['target'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m bunch \u001b[38;5;241m=\u001b[39m fetch_openml(data_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m506\u001b[39m, as_frame\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m boston \u001b[38;5;241m=\u001b[39m bunch\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m----> 8\u001b[0m X \u001b[38;5;241m=\u001b[39m boston\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m boston[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m StandardScaler()\u001b[38;5;241m.\u001b[39mfit_transform(X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   5589\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['target'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "bunch = fetch_openml(data_id=506, as_frame=True)\n",
    "boston = bunch.frame.dropna()\n",
    "X = boston.drop(['target'])\n",
    "y = boston['target']\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "lasso = Lasso()\n",
    "lasso.fit(X_scaled, y)\n",
    "\n",
    "feature_coeffs = pd.Series(lasso.coef_, index=boston.feature_names)\n",
    "print(\"Feature Coefficients: \\n\", feature_coeffs)\n",
    "\n",
    "selected_features= feature_coeffs[feature_coeffs != 0].index.tolist()\n",
    "print('Selected Features: ', selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58891015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
